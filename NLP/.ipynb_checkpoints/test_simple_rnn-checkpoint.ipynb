{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "import unicodedata\n",
    "import string\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import codecs\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def findFiles(path):\n",
    "    \"\"\"\n",
    "    Find files under the given path.\n",
    "    \"\"\"\n",
    "    return glob.glob(path)\n",
    "\n",
    "def unicodeToAscii(s, vocab=string.ascii_letters+\" .,;'\"):\n",
    "    \"\"\"\n",
    "    Turn a unicode string to plain ASCII.\n",
    "    Params:\n",
    "        s: string to be converted\n",
    "        vocab: vocabulary as in unique characters, default to all ascii chars.\n",
    "    \"\"\"\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in vocab\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 1109177 characters,80 unique.\n",
      "{'R': 0, 'L': 1, 'h': 2, '7': 3, '0': 4, 'b': 5, 'W': 6, 'B': 7, 'g': 8, '2': 9, '\\n': 10, 'l': 11, 'J': 12, '6': 13, 'c': 14, ' ': 15, '}': 16, '-': 17, 'I': 18, ')': 19, 'z': 20, 'T': 21, 'e': 22, '?': 23, ':': 24, '\\t': 25, '!': 26, 'd': 27, 'q': 28, 'u': 29, 'N': 30, '\"': 31, 'G': 32, '/': 33, 'S': 34, 'A': 35, ';': 36, 'X': 37, 'k': 38, 'D': 39, 'x': 40, ',': 41, 'Y': 42, 'Z': 43, 'w': 44, 'Q': 45, 'P': 46, 'U': 47, 'a': 48, 'H': 49, '_': 50, '8': 51, 'M': 52, 'm': 53, 's': 54, 'O': 55, '9': 56, 'y': 57, 'i': 58, '3': 59, 'K': 60, 'n': 61, 'p': 62, 'j': 63, 'o': 64, 'V': 65, 'f': 66, '4': 67, 'F': 68, 'r': 69, \"'\": 70, '1': 71, '(': 72, 'E': 73, 't': 74, '.': 75, 'v': 76, '5': 77, '^': 78, 'C': 79}\n",
      "{0: 'R', 1: 'L', 2: 'h', 3: '7', 4: '0', 5: 'b', 6: 'W', 7: 'B', 8: 'g', 9: '2', 10: '\\n', 11: 'l', 12: 'J', 13: '6', 14: 'c', 15: ' ', 16: '}', 17: '-', 18: 'I', 19: ')', 20: 'z', 21: 'T', 22: 'e', 23: '?', 24: ':', 25: '\\t', 26: '!', 27: 'd', 28: 'q', 29: 'u', 30: 'N', 31: '\"', 32: 'G', 33: '/', 34: 'S', 35: 'A', 36: ';', 37: 'X', 38: 'k', 39: 'D', 40: 'x', 41: ',', 42: 'Y', 43: 'Z', 44: 'w', 45: 'Q', 46: 'P', 47: 'U', 48: 'a', 49: 'H', 50: '_', 51: '8', 52: 'M', 53: 'm', 54: 's', 55: 'O', 56: '9', 57: 'y', 58: 'i', 59: '3', 60: 'K', 61: 'n', 62: 'p', 63: 'j', 64: 'o', 65: 'V', 66: 'f', 67: '4', 68: 'F', 69: 'r', 70: \"'\", 71: '1', 72: '(', 73: 'E', 74: 't', 75: '.', 76: 'v', 77: '5', 78: '^', 79: 'C'}\n"
     ]
    }
   ],
   "source": [
    "# IO\n",
    "data = codecs.open('../data/potter.txt', 'r', encoding='utf8', errors='ignore').read()\n",
    "predict = codecs.open('../data/output.txt', 'w', encoding='utf8')\n",
    "chars = list(set(data))\n",
    "data_size = len(data) \n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(f'data has {data_size} characters,{vocab_size} unique.')              # data has 1109177 characters,80 unique.\n",
    "\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "print(char_to_ix)\n",
    "print(ix_to_char)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "hidden_size = 256          # size of hidden layer of neurons\n",
    "seq_length = 128           # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1\n",
    "\n",
    "\n",
    "# model parameters\n",
    "W_xh = torch.randn((hidden_size, vocab_size), requires_grad=True) * 0.01        # weight: input to hidden\n",
    "W_hh = torch.randn(hidden_size, hidden_size, requires_grad=True) * 0.01         # weight: hidden to hidden, notice shape () is omitable\n",
    "W_hy = torch.randn((vocab_size, hidden_size), requires_grad=True) * 0.01        # weight: hidden to output\n",
    "b_h = np.zeros((hidden_size, 1))                          # hidden bias\n",
    "b_y = np.zeros((vocab_size, 1))                           # output bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
