{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "PyTorch: Defining New autograd Functions\n",
    "----------------------------------------\n",
    "\n",
    "A fully-connected ReLU network with one hidden layer and no biases, trained to\n",
    "predict y from x by minimizing squared Euclidean distance.\n",
    "\n",
    "This implementation computes the forward pass using operations on PyTorch\n",
    "Variables, and uses PyTorch autograd to compute gradients.\n",
    "\n",
    "In this implementation we implement our own custom autograd function to perform\n",
    "the ReLU function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 25364118.0\n",
      "1 22642700.0\n",
      "2 23942726.0\n",
      "3 26157952.0\n",
      "4 26384544.0\n",
      "5 22665030.0\n",
      "6 16146236.0\n",
      "7 9713506.0\n",
      "8 5333460.0\n",
      "9 2908031.75\n",
      "10 1704935.125\n",
      "11 1109639.625\n",
      "12 799797.3125\n",
      "13 621511.5\n",
      "14 506767.71875\n",
      "15 425165.25\n",
      "16 362988.0\n",
      "17 313355.46875\n",
      "18 272611.875\n",
      "19 238530.515625\n",
      "20 209694.796875\n",
      "21 185100.046875\n",
      "22 163979.5\n",
      "23 145751.921875\n",
      "24 129963.9765625\n",
      "25 116230.3984375\n",
      "26 104232.859375\n",
      "27 93724.3359375\n",
      "28 84477.8515625\n",
      "29 76312.359375\n",
      "30 69078.203125\n",
      "31 62655.37109375\n",
      "32 56935.08203125\n",
      "33 51832.671875\n",
      "34 47264.96484375\n",
      "35 43167.2890625\n",
      "36 39485.96484375\n",
      "37 36170.37109375\n",
      "38 33181.796875\n",
      "39 30480.556640625\n",
      "40 28033.55078125\n",
      "41 25813.279296875\n",
      "42 23795.767578125\n",
      "43 21959.2578125\n",
      "44 20285.939453125\n",
      "45 18759.2109375\n",
      "46 17364.482421875\n",
      "47 16088.0458984375\n",
      "48 14918.1259765625\n",
      "49 13845.5390625\n",
      "50 12860.7001953125\n",
      "51 11955.0087890625\n",
      "52 11121.3466796875\n",
      "53 10353.423828125\n",
      "54 9644.90234375\n",
      "55 8990.615234375\n",
      "56 8385.9853515625\n",
      "57 7826.70361328125\n",
      "58 7309.61279296875\n",
      "59 6830.861328125\n",
      "60 6386.9296875\n",
      "61 5975.11376953125\n",
      "62 5592.6533203125\n",
      "63 5237.169921875\n",
      "64 4906.59326171875\n",
      "65 4599.05517578125\n",
      "66 4312.6845703125\n",
      "67 4045.792724609375\n",
      "68 3797.4267578125\n",
      "69 3565.83984375\n",
      "70 3349.82080078125\n",
      "71 3148.0283203125\n",
      "72 2959.46875\n",
      "73 2783.128662109375\n",
      "74 2618.24267578125\n",
      "75 2463.904541015625\n",
      "76 2319.41796875\n",
      "77 2184.101806640625\n",
      "78 2057.27294921875\n",
      "79 1938.4256591796875\n",
      "80 1826.9791259765625\n",
      "81 1722.3642578125\n",
      "82 1624.18310546875\n",
      "83 1532.02490234375\n",
      "84 1445.445556640625\n",
      "85 1364.07763671875\n",
      "86 1287.6187744140625\n",
      "87 1215.7362060546875\n",
      "88 1148.1392822265625\n",
      "89 1084.5408935546875\n",
      "90 1024.68603515625\n",
      "91 968.4237060546875\n",
      "92 915.4208374023438\n",
      "93 865.4902954101562\n",
      "94 818.45654296875\n",
      "95 774.1170654296875\n",
      "96 732.3181762695312\n",
      "97 692.9017333984375\n",
      "98 655.7217407226562\n",
      "99 620.630615234375\n",
      "100 587.5241088867188\n",
      "101 556.2657470703125\n",
      "102 526.7603759765625\n",
      "103 498.8904724121094\n",
      "104 472.59539794921875\n",
      "105 447.751708984375\n",
      "106 424.2783203125\n",
      "107 402.09796142578125\n",
      "108 381.1271667480469\n",
      "109 361.3023681640625\n",
      "110 342.55108642578125\n",
      "111 324.817138671875\n",
      "112 308.04150390625\n",
      "113 292.1705017089844\n",
      "114 277.1466369628906\n",
      "115 262.92901611328125\n",
      "116 249.46820068359375\n",
      "117 236.7256317138672\n",
      "118 224.6582794189453\n",
      "119 213.22833251953125\n",
      "120 202.4049072265625\n",
      "121 192.15325927734375\n",
      "122 182.43821716308594\n",
      "123 173.2304229736328\n",
      "124 164.50265502929688\n",
      "125 156.22909545898438\n",
      "126 148.38552856445312\n",
      "127 140.9491424560547\n",
      "128 133.8969268798828\n",
      "129 127.21023559570312\n",
      "130 120.86846923828125\n",
      "131 114.85038757324219\n",
      "132 109.14253234863281\n",
      "133 103.72635650634766\n",
      "134 98.5865707397461\n",
      "135 93.70939636230469\n",
      "136 89.08153533935547\n",
      "137 84.68885040283203\n",
      "138 80.51827239990234\n",
      "139 76.5584716796875\n",
      "140 72.7982177734375\n",
      "141 69.2275619506836\n",
      "142 65.83865356445312\n",
      "143 62.61886215209961\n",
      "144 59.55952453613281\n",
      "145 56.65366744995117\n",
      "146 53.89304733276367\n",
      "147 51.27048873901367\n",
      "148 48.778438568115234\n",
      "149 46.41108703613281\n",
      "150 44.1606559753418\n",
      "151 42.0225944519043\n",
      "152 39.99001693725586\n",
      "153 38.057552337646484\n",
      "154 36.220394134521484\n",
      "155 34.4733772277832\n",
      "156 32.814117431640625\n",
      "157 31.23464584350586\n",
      "158 29.73369789123535\n",
      "159 28.306074142456055\n",
      "160 26.948698043823242\n",
      "161 25.65740203857422\n",
      "162 24.429462432861328\n",
      "163 23.261274337768555\n",
      "164 22.150352478027344\n",
      "165 21.093835830688477\n",
      "166 20.08791160583496\n",
      "167 19.13085174560547\n",
      "168 18.220199584960938\n",
      "169 17.353918075561523\n",
      "170 16.529314041137695\n",
      "171 15.74465560913086\n",
      "172 14.997520446777344\n",
      "173 14.28709888458252\n",
      "174 13.61068058013916\n",
      "175 12.966508865356445\n",
      "176 12.353541374206543\n",
      "177 11.770084381103516\n",
      "178 11.214482307434082\n",
      "179 10.685566902160645\n",
      "180 10.181999206542969\n",
      "181 9.702947616577148\n",
      "182 9.24634075164795\n",
      "183 8.811546325683594\n",
      "184 8.397456169128418\n",
      "185 8.003456115722656\n",
      "186 7.627630233764648\n",
      "187 7.270201206207275\n",
      "188 6.929775714874268\n",
      "189 6.605160236358643\n",
      "190 6.2959818840026855\n",
      "191 6.001678466796875\n",
      "192 5.72132682800293\n",
      "193 5.453914165496826\n",
      "194 5.199501037597656\n",
      "195 4.9570112228393555\n",
      "196 4.7259416580200195\n",
      "197 4.505718231201172\n",
      "198 4.295928955078125\n",
      "199 4.096057891845703\n",
      "200 3.9056949615478516\n",
      "201 3.72428822517395\n",
      "202 3.551271438598633\n",
      "203 3.3865089416503906\n",
      "204 3.229478120803833\n",
      "205 3.079563617706299\n",
      "206 2.936789035797119\n",
      "207 2.8010308742523193\n",
      "208 2.6712441444396973\n",
      "209 2.5475988388061523\n",
      "210 2.4297618865966797\n",
      "211 2.3178060054779053\n",
      "212 2.2106380462646484\n",
      "213 2.108588457107544\n",
      "214 2.0112757682800293\n",
      "215 1.9185564517974854\n",
      "216 1.830129861831665\n",
      "217 1.7457305192947388\n",
      "218 1.6653978824615479\n",
      "219 1.5887153148651123\n",
      "220 1.5156656503677368\n",
      "221 1.4459494352340698\n",
      "222 1.379438042640686\n",
      "223 1.3161619901657104\n",
      "224 1.2557005882263184\n",
      "225 1.1980209350585938\n",
      "226 1.1430853605270386\n",
      "227 1.09067702293396\n",
      "228 1.0406990051269531\n",
      "229 0.9929744005203247\n",
      "230 0.9475064873695374\n",
      "231 0.9041840434074402\n",
      "232 0.8627963662147522\n",
      "233 0.823364794254303\n",
      "234 0.7856349945068359\n",
      "235 0.7497411370277405\n",
      "236 0.7155367732048035\n",
      "237 0.6828529834747314\n",
      "238 0.6517094969749451\n",
      "239 0.6219817996025085\n",
      "240 0.5935881733894348\n",
      "241 0.5665593147277832\n",
      "242 0.5407495498657227\n",
      "243 0.5161072015762329\n",
      "244 0.49257054924964905\n",
      "245 0.470184862613678\n",
      "246 0.4488028287887573\n",
      "247 0.42834538221359253\n",
      "248 0.4089429974555969\n",
      "249 0.3903444707393646\n",
      "250 0.3726401627063751\n",
      "251 0.3557509481906891\n",
      "252 0.33958694338798523\n",
      "253 0.3241487741470337\n",
      "254 0.3094320595264435\n",
      "255 0.29543808102607727\n",
      "256 0.28200939297676086\n",
      "257 0.2692511975765228\n",
      "258 0.25708457827568054\n",
      "259 0.24543233215808868\n",
      "260 0.23436322808265686\n",
      "261 0.2237212210893631\n",
      "262 0.21361398696899414\n",
      "263 0.2039606273174286\n",
      "264 0.19476130604743958\n",
      "265 0.18595664203166962\n",
      "266 0.17755278944969177\n",
      "267 0.16955800354480743\n",
      "268 0.16189587116241455\n",
      "269 0.15455320477485657\n",
      "270 0.1475967913866043\n",
      "271 0.1409521996974945\n",
      "272 0.13457876443862915\n",
      "273 0.1285327523946762\n",
      "274 0.1227332204580307\n",
      "275 0.11719976365566254\n",
      "276 0.11192003637552261\n",
      "277 0.10689303278923035\n",
      "278 0.10209339112043381\n",
      "279 0.09750325977802277\n",
      "280 0.09311272203922272\n",
      "281 0.08893337845802307\n",
      "282 0.08493740111589432\n",
      "283 0.08114176988601685\n",
      "284 0.07750624418258667\n",
      "285 0.0740402415394783\n",
      "286 0.07071475684642792\n",
      "287 0.06755153834819794\n",
      "288 0.064518041908741\n",
      "289 0.061637140810489655\n",
      "290 0.058883119374513626\n",
      "291 0.05624810606241226\n",
      "292 0.053718823939561844\n",
      "293 0.051308583468198776\n",
      "294 0.04901713877916336\n",
      "295 0.04683614522218704\n",
      "296 0.044758327305316925\n",
      "297 0.04275113344192505\n",
      "298 0.04084901511669159\n",
      "299 0.03903065621852875\n",
      "300 0.03728555515408516\n",
      "301 0.03563443198800087\n",
      "302 0.034040506929159164\n",
      "303 0.032521966844797134\n",
      "304 0.03107782080769539\n",
      "305 0.029701601713895798\n",
      "306 0.028379473835229874\n",
      "307 0.02712554857134819\n",
      "308 0.02591315470635891\n",
      "309 0.02476290985941887\n",
      "310 0.0236729197204113\n",
      "311 0.02263072319328785\n",
      "312 0.021623240783810616\n",
      "313 0.020670510828495026\n",
      "314 0.01976398378610611\n",
      "315 0.0188895296305418\n",
      "316 0.01806456781923771\n",
      "317 0.01727205328643322\n",
      "318 0.016514446586370468\n",
      "319 0.015796000137925148\n",
      "320 0.015096682123839855\n",
      "321 0.014444073662161827\n",
      "322 0.013814840465784073\n",
      "323 0.013214812614023685\n",
      "324 0.012636003084480762\n",
      "325 0.01208951510488987\n",
      "326 0.011562544852495193\n",
      "327 0.011060784570872784\n",
      "328 0.010585855692625046\n",
      "329 0.010128672234714031\n",
      "330 0.009694275446236134\n",
      "331 0.0092751644551754\n",
      "332 0.008871047757565975\n",
      "333 0.008492560125887394\n",
      "334 0.008132856339216232\n",
      "335 0.0077865333296358585\n",
      "336 0.00745378527790308\n",
      "337 0.007136656902730465\n",
      "338 0.006834676023572683\n",
      "339 0.006541115231812\n",
      "340 0.006262952461838722\n",
      "341 0.0059995828196406364\n",
      "342 0.005749364849179983\n",
      "343 0.005509804934263229\n",
      "344 0.005281455349177122\n",
      "345 0.0050593167543411255\n",
      "346 0.0048516117967665195\n",
      "347 0.004646999295800924\n",
      "348 0.004449554719030857\n",
      "349 0.00427141971886158\n",
      "350 0.004094512201845646\n",
      "351 0.0039267586544156075\n",
      "352 0.0037663064431399107\n",
      "353 0.0036135700065642595\n",
      "354 0.0034665216226130724\n",
      "355 0.0033265184611082077\n",
      "356 0.0031927907839417458\n",
      "357 0.0030648314859718084\n",
      "358 0.0029420494101941586\n",
      "359 0.002823613816872239\n",
      "360 0.002711063250899315\n",
      "361 0.0026043476536870003\n",
      "362 0.0025041070766747\n",
      "363 0.0024054727982729673\n",
      "364 0.0023108874447643757\n",
      "365 0.002219826215878129\n",
      "366 0.002136620692908764\n",
      "367 0.0020524607971310616\n",
      "368 0.001975573133677244\n",
      "369 0.001899926457554102\n",
      "370 0.0018295914633199573\n",
      "371 0.0017601693980395794\n",
      "372 0.00169339997228235\n",
      "373 0.00162995676510036\n",
      "374 0.0015709878643974662\n",
      "375 0.0015126388752833009\n",
      "376 0.0014582924777641892\n",
      "377 0.0014051332836970687\n",
      "378 0.0013539378996938467\n",
      "379 0.0013058737386018038\n",
      "380 0.0012602090137079358\n",
      "381 0.001216728938743472\n",
      "382 0.0011718766763806343\n",
      "383 0.00113215995952487\n",
      "384 0.0010917091276496649\n",
      "385 0.0010544256074354053\n",
      "386 0.00101856526453048\n",
      "387 0.0009834030643105507\n",
      "388 0.0009499031584709883\n",
      "389 0.0009193022269755602\n",
      "390 0.0008888027514331043\n",
      "391 0.0008588666678406298\n",
      "392 0.0008302103378809988\n",
      "393 0.0008015703642740846\n",
      "394 0.0007769357762299478\n",
      "395 0.0007511044386774302\n",
      "396 0.000727317645214498\n",
      "397 0.000703186437021941\n",
      "398 0.0006806431920267642\n",
      "399 0.0006590079865418375\n",
      "400 0.0006390773924067616\n",
      "401 0.0006194767775014043\n",
      "402 0.0006008376367390156\n",
      "403 0.0005817217752337456\n",
      "404 0.0005640728049911559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "405 0.0005467631854116917\n",
      "406 0.0005301856435835361\n",
      "407 0.0005145389586687088\n",
      "408 0.000498355133458972\n",
      "409 0.00048399457591585815\n",
      "410 0.0004703231679741293\n",
      "411 0.00045593257527798414\n",
      "412 0.00044353908742778003\n",
      "413 0.00043110435944981873\n",
      "414 0.00041907664854079485\n",
      "415 0.00040613001328893006\n",
      "416 0.00039565813494846225\n",
      "417 0.0003846243198495358\n",
      "418 0.0003731827309820801\n",
      "419 0.000363616447430104\n",
      "420 0.00035337015287950635\n",
      "421 0.0003435317485127598\n",
      "422 0.0003341232368256897\n",
      "423 0.00032533484045416117\n",
      "424 0.0003177198814228177\n",
      "425 0.00030870630871504545\n",
      "426 0.00030067600891925395\n",
      "427 0.00029281488968990743\n",
      "428 0.0002856402425095439\n",
      "429 0.0002777108456939459\n",
      "430 0.00027118882280774415\n",
      "431 0.0002640045713633299\n",
      "432 0.0002572576922830194\n",
      "433 0.0002510185295250267\n",
      "434 0.0002444902784191072\n",
      "435 0.0002382281090831384\n",
      "436 0.00023256863642018288\n",
      "437 0.00022660604736302048\n",
      "438 0.0002210495586041361\n",
      "439 0.00021560357708949596\n",
      "440 0.00021034694509580731\n",
      "441 0.0002048760507022962\n",
      "442 0.00020052577019669116\n",
      "443 0.0001958755892701447\n",
      "444 0.00019078727927990258\n",
      "445 0.000186819102964364\n",
      "446 0.000182717849384062\n",
      "447 0.00017860453226603568\n",
      "448 0.00017447293794248253\n",
      "449 0.00017019383085425943\n",
      "450 0.0001666879397816956\n",
      "451 0.00016270042397081852\n",
      "452 0.0001591467298567295\n",
      "453 0.00015592770068906248\n",
      "454 0.00015280516527127475\n",
      "455 0.00014916267537046224\n",
      "456 0.00014568452024832368\n",
      "457 0.0001425840746378526\n",
      "458 0.0001392887788824737\n",
      "459 0.00013626698637381196\n",
      "460 0.00013367659994401038\n",
      "461 0.00013053821749053895\n",
      "462 0.0001274830719921738\n",
      "463 0.00012491180677898228\n",
      "464 0.00012239243369549513\n",
      "465 0.00011956089292652905\n",
      "466 0.00011760312190745026\n",
      "467 0.000115479007945396\n",
      "468 0.00011297464516246691\n",
      "469 0.00011046030704164878\n",
      "470 0.000108501008071471\n",
      "471 0.00010606316936900839\n",
      "472 0.00010388279042672366\n",
      "473 0.00010201997065450996\n",
      "474 9.98610703391023e-05\n",
      "475 9.798965766094625e-05\n",
      "476 9.585226507624611e-05\n",
      "477 9.420175047125667e-05\n",
      "478 9.246725676348433e-05\n",
      "479 9.056863927980885e-05\n",
      "480 8.880645327735692e-05\n",
      "481 8.723214705241844e-05\n",
      "482 8.566572796553373e-05\n",
      "483 8.415569755015895e-05\n",
      "484 8.22733054519631e-05\n",
      "485 8.08033873909153e-05\n",
      "486 7.948483107611537e-05\n",
      "487 7.838410238036886e-05\n",
      "488 7.645142613910139e-05\n",
      "489 7.510749128414318e-05\n",
      "490 7.385016942862421e-05\n",
      "491 7.254514639498666e-05\n",
      "492 7.1304093580693e-05\n",
      "493 6.971820403123274e-05\n",
      "494 6.872382800793275e-05\n",
      "495 6.738651427440345e-05\n",
      "496 6.631074211327359e-05\n",
      "497 6.508903607027605e-05\n",
      "498 6.41158694634214e-05\n",
      "499 6.29931382718496e-05\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class MyReLU(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    We can implement our own custom autograd Functions by subclassing\n",
    "    torch.autograd.Function and implementing the forward and backward passes\n",
    "    which operate on Tensors.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        In the forward pass we receive a Tensor containing the input and return\n",
    "        a Tensor containing the output. ctx is a context object that can be used\n",
    "        to stash information for backward computation. You can cache arbitrary\n",
    "        objects for use in the backward pass using the ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "        with respect to the output, and we need to compute the gradient of the loss\n",
    "        with respect to the input.\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input\n",
    "\n",
    "\n",
    "dtype = torch.float\n",
    "# device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold input and outputs.\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# Create random Tensors for weights.\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # To apply our Function, we use Function.apply method. We alias this as 'relu'.\n",
    "    relu = MyReLU.apply\n",
    "\n",
    "    # Forward pass: compute predicted y using operations; we compute\n",
    "    # ReLU using our custom autograd operation.\n",
    "    y_pred = relu(x.mm(w1)).mm(w2)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # Use autograd to compute the backward pass.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
